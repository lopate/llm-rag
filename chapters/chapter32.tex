\chapter{Математическое ожидание и дисперсия случайной величины, их свойства. Вычисление для нормального распределения.}
\section{Случайные величины}

Пусть дано конечное вероятностное пространство $(\Omega,\mathcal A, \bbP)$, где $\bbP$ "--- вероятности каждого события, состоящего из элементарных исходов из $\Omega$. Тогда \textit{случайной величиной}\rindex{случайная величина} принято называть любую функцию $\xi\colon \Omega \to \bbR$ (т.е.~случайному элементарному исходу ставится в соответствие совершенно конкретное значение).

Абсолютно так же определяется случайная величина для бесконечного счетного вероятностного пространства, где $\Omega = \{w_1,\dots, w_n,\dots\}$.

Пусть теперь дано произвольное вероятностное пространство $(\Omega,\mathcal A, \bbP)$ (т.е.~теперь мы не исключаем непрерывного случая), где снова $\bbP$ "--- вероятности каждого события, состоящих из элементарных исходов из $\Omega$ (вероятностная мера, строго говоря). Тогда в этом общем случае:
\begin{defn}
\textit{Случайной величиной $\xi$}\rindex{случайная величина} называется действительная функция от элементарного события $w$: $\xi = \xi(w)$, $w\in\Omega$, для которой при любом действительном $x$ множество $\{w\colon \xi(w) \le x\}$ принадлежит $\mathcal A$ (т.е. является событием) и для него определена вероятность $\bbP(w\colon \xi(w) \le x)$, записываемая кратко $F_\xi(x)=\bbP(\xi \le x)$. Эта вероятность, рассматриваемая как функция $x$, называется \textit{функцией распределения случайной величины~$\xi$}.
\end{defn}
Отметим ее свойства:
\begin{enumerate}
\item 
$0 \le F(x) \le 1\quad \forall x \in \bboR;$ 
\item
$F(x_1) \le F(x_2)$, если $x_1\le x_2;$
\item
$F(-\infty)=0$, $F(+\infty)=1;$
\item
$F(x+0)=F(x)$ "--- непрерывна справа.\footnote{Заметим, что если бы в определении случайной величины мы положили $F_\xi(x)=\bbP(\xi < x)$ (т.е. строгий знак равенства), то полученная функция распределения была бы непрерывна слева: $F(x-0)=F(x)$. В разных учебниках делают по-разному.}
\end{enumerate}

Важным классом распределений вероятностей являются \textit{абсолютно непрерывные распределения}, для которых существует неотрицательная функция $p(z)$, удовлетворяющая при любых $x$ равенству:
$$F_\xi(x)=\int_{-\infty}^{x}p(z)\,dz,$$ где $p(z)$ называют \textit{плотностью вероятности}\rindex{плотность вероятности}, обладающая следующими свойствами:
\begin{enumerate}
\item 
$p(x)\ge 0,\quad \forall x \in \bbR;$ 
\item 
$\forall x_1,x_2\cquad \bbP(x_1\le\xi<x_2)=\int_{x_1}^{x_2}p(x)\,dx$
\item
$\int_{-\infty}^{+\infty}p(x)\,dx=1$
\end{enumerate}

Другой важный класс составляют \textit{дискретные распределения}, задаваемые конечным или счетным набором вероятностей $\bbP(\xi = x_k)$ для которых
$$
\sum\limits_k \bbP(\xi=x_k)=1,
$$
тогда функция распределения $F_\xi(x)=\sum\limits_{k:\; x_k \le x} \bbP(\xi=x_k)$.

Если распределение случайной величины абсолютно непрерывно или дискретно, то говорят также, что сама случайная величина или ее функция распределения соответственно абсолютно непрерывны или дискретны.

Нужно подчеркнуть, что распределения не делятся лишь только на дискретные и непрерывные. Возможны случаи, когда случайные величины не являются ни дискретными, ни непрерывными (например, взять хотя бы сумму непрерывной и дискретной случайной величины). Но в рамках программы ГОСа мы рассмотрим только отдельно взятые непрерывные и дискретные случайные величины. 

\section{Совместные распределения нескольких случайных величин}
Пусть на вероятностном пространстве заданы случайные величины: $\xi_1$,~$\xi_2$,~\dots,~$\xi_n$.
\begin{defn}
\textit{Совместной функцией распределения} (или \textit{многомерной функцией распределения}) величин $\xi_1,~\xi_2,\dots,\xi_n$ (или случайного вектора $\xi = (\xi_1,\xi_2,\dots,\xi_n)$) называется вероятность 
$$
F_\xi(x)=F_{\xi_1,\dots,\xi_n}(x_1,\dots,x_n)=\bbP(\xi_1 < x_1,\dots,\xi_n<x_n).
$$
\end{defn}

Когда $n$-мерный вектор $(\xi_1,\xi_2,\dots,\xi_n)$ имеет плотностью распределения вероятностей $p(x_1,x_2,\dots,x_n)$, называемой \textit{совместной плотностью распределения}, то функцию распределения можно записать в виде интеграла:
$$
F(x_1,\dots,x_n)=\idotsint\limits_\bbD p(z_1,\dots,z_n)\,dz_1\,dz_2\,\dots\,dz_n,
$$
причем область интегрирования $\bbD$ определяется неравенствами $\xi_i<x_i,\ i\in \overline{1,n}$

Очевидно, что в случае дискретных случайных величин совместную функцию распределения можно записать как n-мерную сумму, также распространенную на область $\bbD$.

Решим задачу о функции распределения суммы непрерывных случайных величин $\zeta=\xi+\eta$. Пусть $p(x_1,x_2)$ "--- плотность распределения вероятностей вектора $(\xi,\eta)$. Искомая функция равна вероятности попадания точки $(\xi,\eta)$ в полупространство $\xi+\eta<x$ и, следовательно, 
\begin{multline}\label{ch31.1eq1}
F_{\zeta}(x)=\iint\limits_{x_1+x_2<x} p(x_1,x_2)\,dx_1\, dx_2=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{x-x_1}p(x_1,x_2)\,dx_1\, dx_2=\\
=\int\limits_{-\infty}^{x}dx_1\int\limits_{-\infty}^{+\infty} p(z,x_1-z)\,dz.
\end{multline}

Сформулируем напоследок одно важное определение.
\begin{defn} Случайные величины $\xi_1$,~\dots,~$\xi_n$ называются \textit{независимыми}, если для любых числовых множеств $B_1,\dots, B_k\ \forall k\in\overline{1,n}$, для которых определены вероятности событий $\{\xi_j\in B_j\}$ имеет место равенство
$$
\bbP(\xi_1\in B_1,\dots, \xi_k \in B_k)=\bbP(\xi_1\in B_1)\cdot\dots\cdot \bbP(\xi_k \in B_k).
$$
\end{defn}

Если положить $B_k=(-\infty;x_k)$, то из только что озвученного определения можно вывести, что для независимых величин верно следующее свойство:
$$
F_{\xi_1,\dots\xi_n}(x_1,\dots,x_n)=F_{\xi_1}(x_1)\dots F_{\xi_n}(x_n).
$$

Отметим, что две дискретные случайные величины $\xi$ и $\eta$ со значениями $x_1,\dots,x_n$ и $y_1,\dots,y_n$ соответственно будут независимы тогда и только тогда, когда для любых $i,j$:
$$
\bbP(\xi = x_i, \eta = y_j) = \bbP (\xi = x_i)\cdot \bbP(\eta=y_j).
$$

Также отметим, что две непрерывные случайные величины $\xi$ и $\eta$ будут независимы тогда и только тогда, когда во всех точках непрерывности функций $p_\xi(x),p_\eta(y),p_{\xi\eta}(x,y)$ "--- плотностей распределений:
$$
p_{\xi\eta}(x,y)=p_\xi(x)\cdot p_\eta(y).
$$ 

\section{Математическое ожидание}

Сначала дадим определение для дискретных случайных величин. Пусть $x_1,x_2,\dots,x_n,\dots$ обозначают возможные значения случайной величины $\xi$, а $p_1,p_2,\dots,p_n,\dots$ "--- соответствующие им вероятности, $\Omega = \{w_1,\dots, w_n,...\}$~"--- наше пространство элементарных исходов в $(\Omega,\mathcal A, \bbP)$. 
\begin{defn}\label{teorver1}
\textit{Математическим ожиданием\rindex{математическое ожидание} дискретной случайной величины} $\xi$ называется число, обозначаемое $\bbE\xi$ и равное
\begin{equation}
\bbE\xi = \sum_{w \in \Omega} \xi(w)\cdot \bbP(w),
\end{equation}
где $\bbP(w)$ "--- элементарные вероятности, если этот ряд сходится абсолютно.
\end{defn}
Перепишем определение матожидания по-другому.
\begin{multline*}
\bbE\xi=\sum_{w \in \Omega} \xi(w)\cdot \bbP(w)=x_1\left(\textstyle\sum\limits_{w\colon \xi(w)=x_1} \bbP(w)\right)+\ldots+x_k\left(\textstyle\sum\limits_{w\colon \xi(w)=x_k} \bbP(w)\right)+\\+\ldots =x_1\bbP(\xi=x_1)+\ldots+ x_k\bbP(\xi=x_k)+\ldots=\sum_{k\in \bbN} x_k\bbP(\xi=x_k)
\end{multline*}
В силу этого равенства, дадим аналогичное определение матожиданию:
\begin{defnn}\label{teorver1s}
\textit{Математическим ожиданием дискретной случайной величины} $\xi$ называется сумма ряда $\sum\limits_{k=1}^{\infty}x_k p_k$, если тот сходится абсолютно.
\end{defnn}

Для непрерывных случайных величин естественным будет следующее определение: 
\begin{defn} 
Если случайная величина $\xi$ непрерывна, $p_{\xi}(x)$ "--- ее плотность распределения и $F_\xi(x)$ "--- функция распределения,  то \textit{математическим ожиданием} \textit{случайной величины} $\xi$ называется интеграл
\begin{equation}\label{ch32:eq:mat_oz_nepr}
\bbE\xi\triangleq\int_{-\infty}^{+\infty} xp_{\xi}(x)\,dx = \int_{-\infty}^{+\infty} x\,dF_\xi(x),
\end{equation}
в тех случаях, когда существует интеграл $\int_{-\infty}^{+\infty} |x|p_{\xi}(x)\,dx.$ 
\end{defn}

\begin{exmpl}
Найти математическое ожидание случайной величины $\xi$, распределенной по нормальному закону
$$
p_{\xi}(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}},
$$
где $\mu,\sigma\in\bbR$ "--- параметры нормального распределения. 
\end{exmpl}
\begin{solution}
По формуле~\eqref{ch32:eq:mat_oz_nepr} находим, что 
$$
\bbE\xi = \int_{-\infty}^{+\infty} x \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \,dx 
$$
Заменой $z = \frac{x-\mu}{\sigma}$ мы приводим вычисляемый интеграл к виду
$$
\bbE\xi = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{+\infty} (\sigma z + \mu) e^{-\frac{z^2}{2}} \,dz = \frac{1}{\sqrt{2\pi}}\left[ \sigma\int_{-\infty}^{+\infty} z e^{-\frac{z^2}{2}} \,dz + \mu\int_{-\infty}^{+\infty} e^{-\frac{z^2}{2}} \,dz\right]
$$
Так как (cмотрите замечание после примера)
$$
\int_{-\infty}^{+\infty} e^{-\frac{z^2}{2}} \,dz = \sqrt{2\pi} \quad\text{и}\quad \int_{-\infty}^{+\infty} z e^{-\frac{z^2}{2}} \,dz = 0
$$
то 
\begin{equation*}
\bbE \xi = \mu.\qedhere
\end{equation*}
\end{solution}

Итак, мы получили важный результат, вскрывающий вероятностный смысл одного и параметров, определяющих нормальное распределение: параметр~$\mu$ в нормальном распределении равен математическому ожиданию.

\begin{notion}
Второй интеграл равен нулю в силу своей нечетности, однако, возможно, стоит напомнить вычисление первого интеграла $I= \int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}} \,dx$, носящего имя \textit{интеграла Гаусса}\rindex{интеграл!Гаусса}.
Гауссов интеграл может быть представлен, как $I = \int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}}\,dx = \int_{-\infty}^{+\infty} e^{-\frac{y^2}{2}}\,dy $. Рассмотрим квадрат этого интеграла $I^2$. Тогда, переходя от двумерных декартовых координат к полярных, получаем:
\begin{multline*}
I^2 = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-\frac{x^2+y^2}{2}}\,dx\,dy = \int_{0}^{2\pi}d\phi \int_{0}^{+\infty} e^{-\frac{r^2}{2}} r\,dr =\\
= 2\pi \int_{0}^{+\infty} e^{-\frac{r^2}{2}} r\,dr 
= 2\pi \int_{0}^{+\infty} e^{-\frac{r^2}{2}}\,d({\scriptstyle \frac{r^2}{2}}) = 2\pi.
\end{multline*}
Следовательно, $I = \int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}}\,dx = \sqrt{2\pi}$.
\end{notion}

\section{Теоремы о математическом ожидании}
\begin{thm}\label{ch31.3thm1}
Математическое ожидание постоянной равно этой постоянной.
\end{thm}
\begin{proof}
Постоянную $C$ мы можем рассматривать, как дискретную случайную величину, которая может принимать только одно значение~$C$ с вероятностью единица; поэтому согласно определению~\ref{teorver1s}:
\begin{equation*}
\bbE C = C\cdot1=C, \qedhere
\end{equation*}
\end{proof}
\begin{thm}\label{ch31.2t1}
Для любых случайных величин $\xi$ и $\eta$, для которых существуют математические ожидания $\bbE\xi$ и $\bbE\eta$ справедливо\footnotemark
\begin{equation}
\bbE(\xi+\eta) = \bbE\xi+\bbE\eta.
\end{equation}
\end{thm}
\footnotetext{Условие на существование математических ожиданий очень важно. Если оно не выполнено, то математическое ожидание суммы не обязано быть равно сумме матожиданий, подобно тому как и сумма двух несходящихся интегралов может неожиданно сойтись.}
\begin{proof}
Рассмотрим сначала случай дискретных случайных величин $\xi$ и $\eta$. Пусть $a_1$,~\dots, $a_n$,~\dots "--- возможные значения величины $\xi$ и $p_1$,~\dots, $p_n$,~\dots "--- их вероятности; $b_1$,~\dots, $b_k$,~\dots "--- возможные значения величины $\eta$ и $q_1$,~\dots, $q_k$,~\dots "--- вероятности этих значений. Возможные значения величины $\xi+\eta$ имеют вид $a_n+b_k$ $(k,n\in\bbN)$. Обозначим через $p_{nk}$ вероятность того, что $\xi$ примет значение $a_n$, а $\eta$ "--- значение $b_k$. По определению математического ожидания
\begin{multline*}
\bbE(\xi+\eta)=\sum\limits_{n,k=1}^{\infty} (a_n+b_k)p_{nk}=\sum\limits_{n=1}^{\infty}\sum\limits_{k=1}^{\infty} (a_n+b_k)p_{nk}=\\
=\sum\limits_{n=1}^{\infty} a_n \left(\sum\limits_{k=1}^{\infty}p_{nk}\right)+\sum\limits_{k=1}^{\infty} b_k \left(\sum\limits_{n=1}^{\infty}p_{nk}\right).
\end{multline*}
Так как по теореме о полной вероятности $\sum\limits_{k=1}^{\infty}p_{nk}=p_n$ и $\sum\limits_{n=1}^{\infty}p_{nk}=q_k$, то 
$$
\sum\limits_{n=1}^{\infty} a_n \left(\sum\limits_{k=1}^{\infty}p_{nk}\right) = \sum\limits_{n=1}^{\infty} a_n p_n = \bbE\xi\ \text{и}\ \sum\limits_{k=1}^{\infty} b_k \left(\sum\limits_{n=1}^{\infty}p_{nk}\right)=\sum\limits_{k=1}^{\infty} b_k q_k = \bbE\eta.
$$
Доказательство теоремы для случая дискретных слагаемых завершено.

Точно так же в случае, когда существуют двумерная плотность распределения $p(x,y)$ случайной величины $(\xi,\eta)$, по формуле \eqref{ch31.1eq1} находим:
\begin{multline*}
\bbE(\xi+\eta)=\int x\,dF_{\xi+\eta}(x)=\int x \left(\int p(z,x-z)\,dz\right)dx=\iint xp(z,x-z)\,dz\, dx=\\=\iint (z+y) p(z,y)\,dz\, dy=\iint z p(z,y)\,dz\, dy + \iint y p(z,y)\,dz\, dy=\\=\int z p_\xi(z)\,dz + \int y p_\eta (y)\,dy= \bbE\xi+\bbE\eta.
\end{multline*}

Эта теорема имеет место и в самом общем случае, но мы не будем его касаться в данном пособии.
\end{proof}

\begin{thm}[мультипликативность]
Математическое ожидание произведения независимых случайных величин $\xi$ и $\eta$, для которых существуют математические ожидания $\bbE\xi$ и $\bbE\eta$ равно произведению их математических ожиданий.
$$
\bbE (\xi\eta)=\bbE\xi\cdot \bbE\eta.
$$
\end{thm}
\begin{proof}
Рассмотрим сначала случай дискретных случайных величин $\xi$ и $\eta$. Пусть $a_1,\dots,a_n,\dots$ "--- возможные значения величины $\xi$ и $p_1,\dots,p_n,\dots$ "--- их вероятности; $b_1,\dots,b_k,\dots$ "--- возможные значения величины $\eta$ и $q_1,\dots,q_k,\dots$ "--- вероятности этих значений. Тогда вероятность того, что $\xi$ примет значение $a_n$, а $\eta$ "--- значение $b_k$ равна $p_n q_k$. По определению математического ожидания 
$$
\bbE(\xi\eta)=\sum\limits_{k,n}a_n b_k p_n q_k = \sum\limits_{n=1}^{+\infty}\sum\limits_{k=1}^{+\infty} a_n b_k p_n q_k =\left(\sum\limits_{n=1}^{+\infty}a_n p_n\right)\left(\sum\limits_{k=1}^{+\infty}b_k q_k\right)=\bbE\xi\cdot \bbE\eta.
$$

Аналогично, если $\xi$ и $\eta$ "--- абсолютно непрерывные случайные величины, и $p_{\xi\eta}(x,y)$ "--- их плотность распределения. Так как они независимы, то
$$
p_{\xi\eta}(x,y)=p_\xi(x)p_\eta(y).
$$
Тогда по формуле математического ожидания для непрерывного случая:
\begin{multline*}
\bbE(\xi\eta)=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}xyp_{\xi\eta}(x,y)\,dx\,dy=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}xyp_\xi(x)p_\eta(y)(x,y)\,dx\,dy=\\=\int\limits_{-\infty}^{+\infty}xp_\xi(x)\,dx\int\limits_{-\infty}^{+\infty}yp_\eta(y)\,dy=\bbE\xi\cdot \bbE\eta.
\end{multline*}

Эта теорема имеет место и в самом общем случае, но мы не будем его касаться в данном пособии.
\end{proof}
\begin{cons}
Математическое ожидание произведения независимых случайных величин $\xi_1,\xi_2,\dots, \xi_n$, для которых существуют математические ожидания $\bbE\xi_1,\bbE\xi_2,\dots, \bbE\xi_n$ равно произведению их математических ожиданий.
$$
\bbE (\xi_1\dots\xi_n)=\bbE\xi_1\cdot\dots\cdot \bbE\xi_n.
$$
\end{cons}

\begin{cons}\label{ch31.2c2}
Постоянный множитель можно выносить за знак математического ожидания
$$
\bbE(C\xi)=C\cdot \bbE\xi
$$
\end{cons}
\begin{proof}
Постоянную $C$ и случайную величину $\xi$ (какой бы она ни была) можно рассматривать как независимые величины.\footnote{Точнее и проще говоря, константа не зависит от элементарных исходов $\Omega$, по которым берется сумма/интеграл в выражении для матожидания, поэтому эту константу можно просто вытащить за знак суммы/интеграла.}
\end{proof}
\begin{cons}[линейность] Для любых случайных величин $\xi_1,\dots,\xi_n$, таких, что существуют математические ожидания $\bbE\xi_1$,~\dots,~$\bbE\xi_n$ и любых чисел $c_1$,~\dots,~$c_n$ справедливо
$$
\bbE(c_1\xi_1+c_2\xi_2+\ldots+c_n\xi_n) = c_1\bbE\xi_1+c_2\bbE\xi_2+\ldots+c_n\bbE\xi_n
$$
\end{cons}
\begin{proof} Это утверждение доказывается по индукции с помощью теоремы \ref{ch31.2t1} и следствия \ref{ch31.2c2}.
\end{proof}
\begin{thm}[монотонность] \label{ch31.2t4}
Если случайные величины $\xi$ и $\eta$ таковы, что $\xi \ge\eta$ и существуют математические ожидания $\bbE\xi$ и $\bbE\eta$, то верно
$$
\bbE\xi\ge \bbE\eta.
$$
\end{thm}
\begin{proof}
Докажем сначала, что из $\xi\ge 0$ следует $\bbE\xi\ge0$. В самом деле, в случае дискретных случайных величин в определении математического ожидания все слагаемые неотрицательны (значения $\xi \ge 0$ и вероятности больше нуля). Аналогично, в случае непрерывных случайных величин в определении математического ожидания подынтегральное выражение неотрицательно (значения $\xi \ge 0$ и плотность больше нуля). Тогда в силу свойства монотонности соответствующих сумм и интегралов получаем, что математическое ожидание $\bbE\xi \ge 0$. Применяя доказанное свойство к неотрицательной разности $\xi-\eta \ge 0$, получаем $\bbE(\xi-\eta)=\bbE\xi-\bbE\eta\ge0$, что и требовалось доказать.

Эта теорема имеет место и в самом общем случае, но мы не будем его касаться в данном пособии.
\end{proof}
\begin{thm}\label{ch31.2t3}
Если случайная величина $\xi$ такова, что $\xi\ge 0$ и $\bbE\xi=0$, то $\xi=0$.
\end{thm}
\begin{proof}
Для дискретных случайных величин из $\bbE\xi=0$ следует, что $\sum\limits_{n=1}^{\infty} x_n p_n = 0$, где $p_n > 0 \ \forall n\in \bbN$ и по условию $x_n \ge 0 \ \forall n\in \bbN$. А значит такое возможно только при $x_n = 0 \; \forall n\in \bbN$.

Аналогично доказывается для непрерывных случайных величин, и эта теорема имеет место быть в самом общем случае.
\end{proof}

\section{Дисперсия}
\begin{defn}
\textit{Дисперсией\rindex{дисперсия} случайной величины $\xi$} называется математическое ожидание квадрата отклонения $\xi$ от $\bbE\xi$, если $\bbE(\xi-\bbE\xi)^2$ существует. Обозначим ее $\bbD\xi$. 
\begin{equation} \label{ch31.4eq1}
\bbD\xi=\bbE(\xi-\bbE\xi)^2.
\end{equation}
\end{defn}

Таким образом, согласно определению формул для  математического ожидания, можно утверждать, что если случайная величина $\xi$ дискретна и имеет закон распределения вероятностей $P(\xi = x_k) = p_k$, $k=\overline{1,\dots}$, где $\sum_{k=1}^{+\infty} p_k = 1$, то ее дисперсия вычисляется по формуле 
\begin{equation}
\bbD\xi = \sum_{i=1}^{+\infty} (x_k - \bbE\xi)^2 p_k;
\end{equation}
для дисперсии непрерывной случайной величины $\xi$ с плотностью распределения $p_{\xi}(x)$ имеем следующую формулу:
\begin{equation}
\bbD\xi = \int_{-\infty}^{+\infty} (x - \bbE\xi)^2 p_{\xi}(x)\,dx.
\end{equation}

\begin{exmpl}
Найти дисперсию случайной величины $\xi$, распределенной по нормальному закону
$$
p_{\xi}(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}},
$$
где $\mu,\sigma\in\bbR$ "--- параметры нормального распределения. 
\end{exmpl}
\begin{solution}
Мы уже знаем, что $\bbE\xi = \mu$, поэтому
$$
\bbD\xi = \int_{-\infty}^{+\infty} (x - \mu)^2 p_{\xi}(x) \,dx = \frac{1}{\sigma \sqrt{2\pi}}\int_{-\infty}^{+\infty} (x - \mu)^2 e^{-\frac{(x-\mu)^2}{2\sigma^2}} \,dx
$$
Заменой $z = \frac{x-\mu}{\sigma}$ мы приводим вычисляемый интеграл к виду
$$
\bbD\xi = \frac{\sigma^2}{ \sqrt{2\pi}}\int_{-\infty}^{+\infty} z^2 e^{-\frac{z^2}{2}} \,dz
$$
Интегрированием по частям находим, что 
$$
\frac{\sigma^2}{ \sqrt{2\pi}}\int_{-\infty}^{+\infty} z^2 e^{-\frac{z^2}{2}} \,dz = \frac{\sigma^2}{ \sqrt{2\pi}}\left[-z e^{-\frac{z^2}{2}} \bigg|_{-\infty}^{+\infty} + \int_{-\infty}^{+\infty} e^{-\frac{z^2}{2}}\,dz\right]
$$
Первое слагаемое в квадратных скобках равно $0$, а второе представляет из себя упомянутый ранее интеграл Гаусса, который равен $\sqrt{2\pi}$. Таким образом, окончательно
\begin{equation*}
\bbD\xi = \sigma^2.\qedhere
\end{equation*}
\end{solution}

Мы выяснили, таким образом, вероятностный смысл второго парамера, определяющего нормальный закон. Мы видим, что нормальный закон полностью определен математическим ожиданием и дисперсией. 

Итак, дисперсия играет роль меры рассеяния (разбросанности) значений случайной величины около математического ожидания.

Заметим, что в силу линейности математического ожидания и теоремы~\ref{ch31.3thm1} и т.к.~математическое ожидание по сути это постоянное число (точнее предел), то $\bbE(\bbE\xi)=\bbE\xi$:
\begin{multline*}
\bbD\xi = \bbE(\xi-\bbE\xi)^2=\bbE(\xi^2-2\xi \bbE\xi+(\bbE\xi)^2)=\bbE\xi^2-2\bbE\xi \bbE\xi+(\bbE\xi)^2=\\=\bbE\xi^2-(\bbE\xi)^2
\end{multline*}
Так как дисперсия является неотрицательной величиной (покажем ниже), то из последнего мы выводим одно свойство матожидания: $\bbE\xi^2\ge(\bbE\xi)^2$.



Отметим основные свойства дисперсии.

\begin{thm} Дисперсия любой случайной величины неотрицательна, причем $\bbD\xi = 0$ тогда и только тогда, когда $\xi$ "--- постоянная. 
\end{thm}
\begin{proof}
Свойство неотрицательности следует из неравенства $(\xi - \bbE \xi)^2 \ge 0$ и свойства монотонности математического ожидания: $\bbD \xi = \bbE (\xi - \bbE \xi)^2 \ge 0$.

Если $\xi = c$, $c$ "--- постоянная, то $\bbD c = \bbE (c - \bbE c)^2 = 0$. 

Если $\bbD\xi=\bbE (\xi - \bbE \xi)^2=0$, то учитывая, что $(\xi - \bbE \xi)^2\ge 0$ из теоремы \ref{ch31.2t3} получаем, что $(\xi - \bbE \xi)^2=0$, т.е.~$\xi=\bbE\xi$, а так как математическое ожидание "--- постоянное число, то мы доказали нашу теорему в обе стороны.
\end{proof}
\begin{thm} Если $a$ "--- постоянная, то $$\bbD(a\xi) = a^2 \bbD\xi.$$
\end{thm}
\begin{proof}
Действительно, 
\begin{equation*}
\bbD(a\xi) = \bbE[a\xi - \bbE (a\xi)]^2 = \bbE [a (\xi - \bbE \xi)]^2 = a^2\bbE(\xi - \bbE\xi)^2 = a^2 \bbD\xi. \qedhere
\end{equation*}
\end{proof}

\begin{thm}\label{ch31.disp1}
Если случайные величины $\xi$ и $\eta$ независимы, то $$\bbD(\xi + \eta) = \bbD\xi + \bbD\eta.$$
\end{thm}
\begin{proof}
Используя определение дисперсии $\eqref{ch31.4eq1}$ и свойство линейности математического ожидания, получим
$$
\bbD(\xi + \eta) = \bbE[(\xi + \eta) - \bbE(\xi + \eta)]^2 = \bbE(\xi - \bbE\xi)^2 + 2\bbE(\xi - \bbE\xi)(\eta - \bbE\eta) + \bbE(\eta  - \bbE\eta)^2.
$$

Отсюда следует формула из теоремы~\ref{ch31.disp1}, так как согласно свойству мультипликативности математического ожидания
\begin{equation*}
\bbE(\xi - \bbE\xi)(\eta - \bbE\eta) = \bbE(\xi - \bbE\xi)\bbE(\eta - \bbE\eta) = (\bbE\xi - \bbE\xi)(\bbE\eta - \bbE\eta) = 0. \qedhere
\end{equation*}
\end{proof}

Формула из теоремы~\ref{ch31.disp1} по индукции распространяется на сумму~$n$ попарно независимых случайных величин. 
\begin{cons}\label{ch31.disp13}
Если $\xi_1$,~$\xi_2$, \ldots,~$\xi_n$ попарно независимы, то 
$$
\bbD(\xi_1 + \xi_2 + \ldots + \xi_n) = \bbD(\xi_1) + \bbD(\xi_2) + \ldots + \bbD(\xi_n).
$$
\end{cons}

\section{Ковариация}
\begin{defn}
Величина $\bbE[(\xi - \bbE\xi)(\eta - \bbE\eta)]$ носит название \textit{ковариации}\rindex{ковариация} между $\xi$ и $\eta$ и обозначается $$\cov (\xi,\eta) = \bbE[(\xi - \bbE\xi)(\eta - \bbE\eta)].$$
\end{defn}

Теперь можно обобщить следствие~\ref{ch31.disp13} на случай зависимых случайных величин.
\begin{thm}
Имеет место формула
$$
\bbD(\xi_1 + \xi_2 + \ldots + \xi_n) = \sum\limits_{k=1}^{n} \bbD(\xi_k) + 2 \sum\limits_{1\le k < l \le n} \cov (\xi_k,\xi_l).
$$
\end{thm}
\begin{proof}
Доказательство для двух случайных величин ничем не отличается от данного в теореме \ref{ch31.disp1}, кроме того, что замечаем  $\bbE(\xi - \bbE\xi)(\eta - \bbE\eta)=\cov (\xi,\eta).$ По индукции верно и для произвольного натурального числа $n$.
\end{proof}
\begin{thm}[Неравенства Коши"--~Буняковского\rindex{неравенство!Коши"---Буняковского}]
Для любых двух случайных величин $\xi, \eta$
\begin{align}
&|\bbE(\xi\eta)| \le \sqrt{\bbE\xi^2 \cdot \bbE\eta^2}.\\
&|\cov(\xi,\eta)| \le \sqrt{\bbD\xi\cdot \bbD\eta}.\label{chHZeq17}
\end{align}
\end{thm}
\begin{proof}
Для любых чисел $x, y$ по теореме \ref{ch31.2t4} о математическом ожидании 
$$
\bbE(x\xi + y\eta)^2 \ge 0.
$$

Отсюда следует, что квадратичная формула
$$
x^2\bbE\xi^2 + 2xy\bbE\xi\eta + y^2\bbE\eta^2
$$
неотрицательно определена, а следовательно, ее дискриминант неположителен:
$$
(\bbE(\xi\eta))^2 - \bbE\xi^2 \cdot \bbE\eta^2 \le 0.
$$

Аналогично можно доказать второе неравенство~\eqref{chHZeq17} Коши"--~Буняковского, взяв в качестве неотрицательной функции $\bbD(x\xi + y\eta)\ge 0$.\footnote{Кстати, второе неравенство КБ является более естественным, потому что именно ковариация играет роль скалярного произведения в пространстве случайных величин с конечным вторым моментом. Неравенство КБ "--- неотъемлемое свойство \underline{именно} скалярного произведения.}
\end{proof}

\begin{defn}
Величина $\displaystyle\frac{\cov(\xi,\eta)}{\sqrt{\bbD\xi\cdot \bbD\eta}}$ называется коэффициентом корреляции\rindex{коэффициент!корреляции} между $\xi$ и $\eta$  и обозначается $$\rho (\xi,\eta) = \frac{\cov(\xi,\eta)}{\sqrt{\bbD\xi\cdot \bbD\eta}}.$$
\end{defn}

Обсудим пару свойств коэффициента корреляции.
\begin{itemize}
\item 
$|\rho(\xi,\eta)|\le1$.

Это следует из неравенства Коши"--~Буняковского $|\cov(\xi,\eta)| \le \sqrt{\bbD\xi\cdot \bbD\eta}$.
\item
Если $\xi$, $\eta$ независимы, то $\rho(\xi,\eta)=0$.

Это было попутно получено в теореме~\ref{ch31.disp1}. Повторим еще раз. В случае независимых $\xi$ и $\eta$
\begin{multline*}
\cov(\xi,\eta)=\bbE(\xi - \bbE\xi)(\eta - \bbE\eta) = \bbE(\xi - \bbE\xi)\bbE(\eta - \bbE\eta)=\\ = (\bbE\xi - \bbE\xi)(\bbE\eta - \bbE\eta) = 0.
\end{multline*}
\end{itemize}
