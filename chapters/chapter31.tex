\part{Теория вероятностей}

\chapter{Полная система событий. Формула полной вероятности. Формула Байеса. Независимость событий и классов событий.}

\section[Классическое определение вероятности. Интуитивные понятия о вероятности.]{Классическое определение вероятности. Интуитивные понятия о вероятности.\protect\footnote{Также рекомендую посмотреть сборник определений и формулировок теорем, которые Максим Широбоков выложил по этой ссылке \href{https://vk.com/wall5284431_976}{$vk.com/...$}}}

Рассмотрим обычную игральную кость "--- кубик, на каждой из шести граней которого нанесены числа от 1 до 6. У нас есть всего 6 вариантов того, как этот кубик может упасть на стол после случайного бросания Какая же цифра выпадет на кубике? 
\begin{center}
1, 2, 3, 4, 5, 6 "--- всего 6 исходов нашего испытания.
\end{center}
Каким свойствами обладают эти варианты?
\begin{enumerate}
\item Хотя бы один из исходов обязательно случится. (т.е.~на формальном языке эти исходы образуют полную группу событий )
\item Никакие два одновременно не происходят. (попарно несовместны)
\item Исходы равновозможны. (равновероятны)
\end{enumerate}

Подобных примеров можно придумать великое множество. Например, пусть есть игральная колода из $36$ карт. А исход "--- взаимное расположение карт друг за другом после тщательной перетасовки (их $36!$ всего способов переставить карты внутри множества от~$1$ до~$36$). Такая система событий тоже обладает перечисленными свойствами.

Пусть в рамках какого-то эксперимента есть какие-то исходы $w_1$,~\dots, $w_n$, и они обладают тремя перечисленными выше свойствами. События, состоящее из одного исхода, станем называть \textit{элементарными событиями}. Тогда по определению считают\footnote{С современной и строгой точки зрения (Колмогорова) вероятность определяется на множествах, не на исходах, поэтому правильнее было бы элементарное событие обозначить $\{w_i\}$,т.е.~как множество, состоящее из одного элемента, а вероятность $\bbP(\{w_i\})$. Но в рамках данной книги я допущу себе эту вольность, чтобы не запутываться со строгостью изложения.}
$$
\bbP(w_i)=\frac{1}{n}.
$$
где $\bbP(w_i)$ "--- вероятность произвольного элементарного события $w_i$. Действительно, если хотя бы один из этих исходов произойдет, причем на самом деле ровно один и эти исходы равновозможны, естественно сказать, что вероятность каждого их этих исходов "--- это $\frac{1}{n}$. Тогда в задаче про кубик $\bbP(w_i) =\frac{1}{6}$, а в задаче про карты $\bbP(w_i) = \frac{1}{36!}$.

Наряду с элементарными событиями рассматриваются также случайные события, ведь часто представляет интерес наступление при испытании не какого-то элементарного события, а одного из нескольких элементарных событий.  Например, в качестве события в задаче про игральную кость можно рассмотреть $A$ "--- игральная кость выпала четной стороной кверху. Это, очевидно, означает, что она выпала либо стороной~2, либо стороной~4, либо стороной~6. То есть, как говорят, элементарных исходов, которые благоприятствуют этому \textit{случайному событию} $A$ "--- их всего 3 штуки. И все эти элементарные исходы равновероятны. Событие $A$ с точки зрения множеств считают множеством из всех элементарных исходов $w_{i_1}$,~\dots, $w_{i_k}$, ему благоприятствующих. Тогда
\begin{defn}[Классическое определение вероятности]
\textit{Вероятностью случайного события $A$}, обозначаемой $\bbP(A)$, называется отношение 
$$
\bbP(A)=\frac{\left\{\parbox{7cm}{\centering число несовместимых и равновозможных\\ элементарных событий, составляющих \textit{А}}\right\}}{\{\parbox{7cm}{число всех возможных элементарных событий}\}}=\frac{k}{n}
$$ 
\end{defn}

В нашем случае $\bbP(A)=\frac{3}{6}=\frac{1}{2}$.

Станем рассматривать некоторую систему событий $A,B,C\dots $, каждое из которых должно \textit{произойти} или \textit{не произойти}. Тогда заметим, что вероятность, определенная нами в таком смысле, обладает следующими свойствами.
\begin{enumerate}
\item 
Для каждого события $\bbP(A)=\frac{k}{n}\ge 0$.
\item \label{ch30r2}
Обозначим за $\Omega \triangleq \{w_1, \dots,w_n\}$ "--- \textit{множество (пространство) всех элементарных исходов}. Тогда, очевидно, $\bbP(\Omega)=\frac{n}{n}=1$. Будем называть событие $\Omega$ \textit{достоверным}. 

\item 
$\bbP(A\sqcup B)=\frac{k'+k''}{n}=\frac{k'}{n}+\frac{k''}{n}=\bbP(A)+\bbP(B)$ "--- вероятность дизъюнктного объединения двух событий (т.е.~события предполагаются непересекающимися "--- элементарные исходы, благоприятствующие событию $A$, и исходы, благоприятствующие событию $B$, представляют собой непересекающиеся множества) равна сумме вероятностей события $A$ и события $B$.
\item \label{ch30r5}
$\bbP(A\cup B)=\bbP(A)+\bbP(B)-\bbP(A\cap B)$ "--- вероятность обычного объединения (предполагаем, что пересечения тоже могут быть "--- есть какие-то исходы, которые благоприятствуют и событию $A$, и событию $B$), т.е.~тех событий, которые благоприятствуют или событию A, или событию B. Если $A\cap B=\emptyset$, то события называют \textit{несовместимыми}. И тогда, как и в предыдущем пункте, $\bbP(A\cup B)=\bbP(A)+\bbP(B)$
\item
Будем обозначать любое \textit{невозможное} событие $\emptyset$. Тогда $\bbP(\emptyset)=0$. 

\item
$\overline{A}=\Omega\setminus A$~"--- \textit{отрицание события $A$}. Событие, \textit{противоположное A}, "--- это событие, которому благоприятствуют те исходы, которые не благоприятствуют событию $A$. 

Тогда $\bbP(\overline{A})=1-\bbP(A)$.
\begin{proof}
$\overline{A}\cup A=\Omega\xrightarrow{\ref{ch30r2}}\bbP(A\cup\overline{A}) = 1$, а так как $\overline{A}$ и $A$ несовместны, то по свойству \ref{ch30r5}:  $\bbP(A)+\bbP(\overline{A}) = 1$.
\end{proof}

\item
Если событие $A$ влечет за собой событие $B$ (т.е. множество $A$ является подмножеством множества $B$), то $\bbP(A)\le \bbP(B)$.
\begin{proof}
$\bbP(B)=\bbP(A\cup (\overline{A}\cap B))=\bbP(A)+\bbP(\overline{A}\cap B)\ge \bbP(A).$
\end{proof}

\item
$\bbP(A_1\cup\dots\cup A_k) \le \bbP(A_1)+\ldots+\bbP(A_k)$.

\item Формула вклю\-чений-исключений\rindex{формула!включений-исключений}:
\begin{multline*}
\bbP(A_1\cup\dots\cup A_k)= \bbP(A_1)+\ldots+\bbP(A_k)-\bbP(A_1\cap A_2)-\\-\dots -\bbP(A_{k-1}\cap A_{k})+ \dots + (-1)^{k-1}\bbP(A_1\cap\dots\cap A_k)
\end{multline*}
Это полный аналог формулы вклю\-чений--исключений из комбинаторики, поэтому мы ее не доказываем.
\end{enumerate}

\begin{defn}
События $A_1$,~$A_2$, \dots,~$A_n$ образуют \textit{полную группу событий}, если хотя бы одно из них непременно должно произойти, т.е.~если
$$
A_1\cup A_2\cup\dots\cup A_n=\Omega.
$$
\end{defn}
Для нее верны все свойства, указанные выше.
\section{Аксиоматическое определение вероятности А.Н.~Колмогорова}

Аксиоматическое определение вероятности включает в себя как частные случаи классическое определение вероятности, которое мы обсудили в прошлом параграфе, и статистическое (в котором сначала проводят $n$ испытаний, и при $k$ успешных испытаниях считают $\bbP=\lim_{n \to +\infty}\limits\frac{k(n)}{n}$) и преодолевает недостаточность каждого из них.

Отправным пунктом аксиоматики Колмогорова является множество $\Omega$, элементы которого называются \textit{элементарными событиями}. Наряду с $\Omega$ рассматривается множество $\mathfrak{F}$ подмножеств $\Omega$ "--- множества элементарных событий. Элементы $\mathfrak{F}$ называются \textit{случайными событиями.}
\begin{defn} Множество $\mathfrak{F}$ называется \textit{алгеброй множеств}, если выполнены следующие требования:
\begin{enumerate}
\item
$\Omega \in \mathfrak{F},\; \varnothing \in \mathfrak{F} \;(\varnothing - \text{пустое множество});$
\item
из того, что $A \in \mathfrak{F}$, следует, что так же $\overline{A} \in \mathfrak{F}$;
\item
из того, что  $A \in \mathfrak{F}$ и $B \in \mathfrak{F}$ , следует, что $A \cup B \in \mathfrak{F}$ и $A \cap B \in \mathfrak{F}$.

Если дополнительно к перечисленным выполняется еще следующее требование:
\item
из того, что $A_n \in \mathfrak{F}$ (при $n = 1,2, \ldots$), вытекает, что $\bigcup\limits_{n} A_n \in \mathfrak{F}$ и $\bigcap\limits_{n} A_n \in \mathfrak{F}$, то множество $\mathfrak{F}$ называется \textit{$\sigma$-алгеброй}. 
\end{enumerate}
\end{defn}

Под операциями над случайными событиями понимаются операции над соответствующими множествами. 

Теперь мы можем перейти к формулировке аксиом, определяющих вероятность.
\begin{axiome} 
Каждому случайному событию $A$ поставлено в соответствие неотрицательное число $\bbP(A)$, называемое его вероятностью.
\end{axiome}
\begin{axiome} \label{armyman1}
$\bbP(\Omega) = 1$
\end{axiome}
\begin{axiome}[Расширенная аксиома сложения]
Если событие $A$ равносильно наступлению хотя бы одного из попарно несовместимых событий $A_1$,~$A_2$, \ldots,~$A_n$, \ldots, то
$$
\bbP(A) = \bbP(A_1) + \bbP(A_2) + \ldots + \bbP(A_n) + \ldots
$$
\end{axiome}
\begin{cons}[Теорема сложения]\label{armyman2} 
Если события $A_1$,~$A_2$,~\ldots, $A_n$ попарно несовместимы, то 
$$
\bbP(A_1 \cup A_2 \cup \ldots \cup A_n) = \bbP(A_1) + \bbP(A_2) + \ldots + \bbP(A_n).
$$
\end{cons}
Для классического определения вероятности свойства, выраженные аксиомой~\ref{armyman1}, не нужно было постулировать, так как это свойство вероятности было нами доказаны.

Из сформулированных аксиом мы выведем несколько важных элементарных следствий.

Прежде всего, из очевидного равенства
$$
\Omega = \varnothing + \Omega
$$

и следствия~\ref{armyman2} мы заключаем, что
$$
\bbP(\Omega) = \bbP(\varnothing) + \bbP(\Omega).
$$

Таким образом.
\begin{enumerate}
\item
Вероятность невозможного события равна нулю.
\item	
Для любого события $A$
$$
\bbP(\overline{A}) = 1 - \bbP(A).
$$
\item
Каково бы ни было случайное событие $A$,
$$
0 \le \bbP(A) \le 1.
$$
\item
Если событие $A$ влечет за собой событие $B$, то
$$
\bbP(A) \le \bbP(B).
$$
\item
Пусть $A$ и $B$ "--- два произвольных события. Поскольку в суммах $A \cup B = A \cup (B \setminus (A\cap B))$ и $B = A\cap B \cup (B \setminus (A\cap B))$ слагаемые являются несовместными событиями, то в соответствии с аксиомой 3
$$
\bbP(A \cup B) = \bbP(A) + \bbP(B \setminus A\cap B);\quad \bbP(B) = \bbP(A\cap B) + \bbP(B \setminus A\cap B).
$$
\end{enumerate}

Отсюда вытекает теорема сложения для произвольных событий $A$ и $B$
$$
\bbP(A \cup B) = \bbP(A) + \bbP(B) - \bbP(A\cap B).
$$
В силу неотрицательности $\bbP(A\cap B)$ отсюда заключаем, что
$$
\bbP(A \cup B) \le \bbP(A) + \bbP(B).
$$
По индукции теперь выводим, что если $A_1$,~$A_2$, \ldots,~$A_n$ "--- произвольные события, то имеет место неравенство
$$
\bbP\{ A_1 \cup A_2 \cup \ldots \cup A_n \} \le \bbP(A_1) + \bbP(A_2) + \ldots + \bbP(A_n).
$$

Вероятностным пространством принято называть тройку символов $(\Omega, \mathfrak{F}, \bbP)$, где $\Omega$ "--- множество элементарных событий, $\mathfrak{F}$ — $\sigma$-алгебра подмножеств $\Omega$, называемых случайными событиями, и $\bbP(A)$ --— вероятность, определенная на $\sigma$-алгебре $\mathfrak{F}$.

\section{Условная вероятность, независимость событий}

Однако в ряде случаев приходится рассматривать вероятности событий при дополнительном условии, что произошло некоторое событие В. Такие вероятности мы будем называть \textit{условными} и обозначать символом $\bbP(A|B)$: это означает вероятность события $A$ при условии, что событие $B$ произошло.
 
Решим задачу нахождения условной вероятности для классического определения вероятности.

Пусть есть множество элементарных исходов $\Omega=\{w_1,\dots w_n\}$, событие $B=\{w_{i_1},\dots w_{i_k}\} \subseteq \Omega$. И есть еще событие $A$, вероятность которого мы хотим посчитать, при том условии, что событие $B$ произошло, т.е.~произошел ровно один из $|B|=k$ элементарных исходов, и $|B|$ надо поставить в знаменатель (столько всего возможных исходов в нашем испытании). А в числитель надо поставить количество тех благоприятствующих событию $B$ исходов, которые, в свою очередь, благоприятствуют событию $A$, т.е. найти исходы, которые благоприятствуют обоим событиям, что, естественно, равняется $|A\cap B|$.
\begin{equation} \label{ch30.2eq1}
\bbP(A|B) = \frac{|A\cap B|}{|B|} = \frac{|A\cap B|/|\Omega|}{|B|/|\Omega|} = \frac{\bbP(A\cap B)}{\bbP(B)}.
\end{equation}

Понятно, что если $B$ "--- событие, для которого $\bbP(B)=0$, то равенство~$\eqref{ch30.2eq1}$ теряет смысл.

Заметим, что рассуждения, проведенные нами, не являются доказательством, а представляют только мотивировки следующего определения.
Формула~$\eqref{ch30.2eq1}$, которая в случае классического определения была нами выведена из определения условной вероятности, в случае аксиоматического определения вероятности будет взята нами в качестве определения. 
\begin{defn} В общем случае при $\bbP(B) > 0$ число $\bbP (A|B)$, определяемое равенством
$$
\bbP(A|B) = \frac{\bbP(A\cap B)}{\bbP(B)}.
$$
называется \textit{условной вероятностью} события $A$ при условии $B$.
\end{defn}

При $\bbP(A)\bbP(B) > 0$ равенство $\eqref{ch30.2eq1}$ эквивалентно так называемой \textit{теореме умножения}, согласно которой

\begin{equation} \label{ch30.2eq2}
\bbP(A\cap B) = \bbP(B|A)\bbP(A) = \bbP(A|B)\bbP(B),
\end{equation}
т.е.~\textit{вероятность произведения двух событий равна произведению вероятностей одного из этих событий на условную вероятность другого при условии, что первое произошло}.

Теорема умножения применима и в том случае, когда для одного из событий $A$ или $B$ вероятность равна нулю, так как в этом случае вместе, например, с $\bbP(A) = 0$ имеют место равенства $\bbP(A|B) = 0$ и $\bbP(A\cap B) = 0$.

Вполне естественно, говорят, что событие $A$ \textit{независимо} от события $B$, если имеет место равенство

\begin{equation} \label{ch30.2eq3}
\bbP(A|B) = \bbP(A),
\end{equation}
т.е.~если наступление события $B$ не изменяет вероятности события $A$. Если событие $A$ независимо от $B$, то в силу $\eqref{ch30.2eq2}$ имеет место равенство
$$
\bbP(A)\bbP(B|A) = \bbP(B)\bbP(A).
$$

Отсюда при $\bbP(A) > 0$ находим, что
\begin{equation} \label{ch30.2eq4}
\bbP(B|A) = \bbP(B).
\end{equation}
т.е.~событие $B$ также, т.е.~независимо от $A$. Таким образом, свойство независимости событий \textit{взаимно}.

Для независимых событий теорема умножения принимает особенно простой вид, а именно, если события $A$ и $B$ независимы, то
$$
\bbP(A\cap B) = \bbP(A) \cdot \bbP(B).
$$

Если независимость событий $A$ и $B$ определить посредством последнего равенства, то это определение верно всегда, в том числе и тогда, когда $\bbP(A) = 0$ или $\bbP(B) = 0$. Поэтому
\begin{defn}
События $A$ и $B$ \textit{независимы}, если выполняется $$\bbP(A\cap B) = \bbP(A)\cdot \bbP(B).$$
\end{defn}

События $B_1$,~$B_2$,~\dots,~$B_s$ называют \textit{независимыми в совокупности}, если для любого события $B_p$ из их числа и произвольных $B_{i_1}$,~$B_{i_2}$,~\dots,~$B_{i_r}$ из их же числа и отличных от $B_p$  ($i_n \not= p$  и $1 \le n \le r$) события $B_p$ и $B_{i_1}\cap B_{i_2}\cap \dots\cap B_{i_r}$ взаимно независимы.

В силу предыдущего, это определение эквивалентно следующему:
\begin{defn} События $B_1$,~$B_2$,~\dots,~$B_s$ называют \textit{независимыми в совокупности}, если при любых $1 \le i_1 < i_2 < \dots < i_r \le s$ и $r$ $(1 \le r \le s)$
\begin{equation}\label{ch31:def:nez_sob_sov}
\bbP(B_{i_1}\cap B_{i_2}\cap\ldots\cap B_{i_r}) = \bbP(B_{i_1})\bbP(B_{i_2}) \ldots \bbP(B_{i_r}).
\end{equation}
\end{defn}
Если~\eqref{ch31:def:nez_sob_sov} выполнено только при $s=2$, то события называют \textit{попарно независимыми}. Как видно, для независимости в совокупности нескольких событий недостаточно их попарной независимости. Однако из независимости в совокупности вытекает попарная независимость, потройная, и т.д.

\section{Формула полной вероятности}
Предположим теперь, что событие $B$ может осуществиться с одним и только с одним из $n$ несовместимых событий $A_1,A_2, \ldots, A_n$. Иными словами, положим, что
\begin{equation} \label{ch30.2eq5}
B = \bigcup\limits_{i = 1}^{n} B \cap A_i,
\end{equation}
где события $B\cap A_i$ и $B\cap A_j$ с разными индексами $i$ и $j$ несовместимы. По теореме сложения вероятностей имеем:
$$
\bbP(B) = \sum_{i = 1}^{n} \bbP(B\cap A_i).
$$

Использовав теорему умножения, находим, что
\begin{equation}\label{fullprob}
\bbP(B) = \sum_{i = 1}^{n} \bbP(B|A_i)\bbP(A_i).
\end{equation}
Это равенство носит название \textit{формулы} \hyperref[fullprob]{\textit{полной вероятности}}\rindex{формула!полной вероятности}.

\section{Формула Байеса}
Пусть по-прежнему имеет место равенство~\eqref{ch30.2eq5}. Требуется найти вероятность события $A_i$, если известно, что $B$ произошло. Согласно теореме умножения имеем:
$$
\bbP(A_i\cap B) = \bbP(B)\bbP(A_i|B) = \bbP(A_i)\bbP(B|A_i).
$$

Отсюда
$$
\bbP(A_i|B) = \frac{\bbP(A_i)\bbP(B|A_i)}{\bbP(B)},
$$
используя формулу \hyperref[fullprob]{полной вероятности}, находим, что
\begin{equation}\label{Bayes}
\bbP(A_i|B) = \frac{\bbP(A_i)\bbP(B|A_i)}{\sum\limits_{j = 1}^{n} \bbP(A_j)\bbP(B|A_j)}.
\end{equation}
Два последних равенства носят название \textit{формул} \hyperref[Bayes]{\textit{Байеса}}\rindex{формула!Байеса}. Формулы \hyperref[Bayes]{Байеса} часто интерпретируются как формулы, позволяющие по \textit{априорным}\rindex{вероятность!априорная} (известным предварительно, до проведения опыта) вероятностям $P(A_i)$ и по результатам опыта (наступления события $B$) найти \textit{апостериорные}\rindex{вероятность!апостериорная} (вычисленные после опыта) вероятности $P(A_i|B)$.
